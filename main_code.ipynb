{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitz in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: configobj in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (7.1.0)\n",
      "Requirement already satisfied: httplib2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (5.3.2)\n",
      "Requirement already satisfied: nipype in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (1.9.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (2.2.3)\n",
      "Requirement already satisfied: pyxnat in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (1.6.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fitz) (1.13.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httplib2->fitz) (3.2.0)\n",
      "Requirement already satisfied: packaging>=20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nibabel->fitz) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nibabel->fitz) (4.12.2)\n",
      "Requirement already satisfied: click>=6.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (8.1.7)\n",
      "Requirement already satisfied: networkx>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (3.3)\n",
      "Requirement already satisfied: prov>=1.5.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (2.0.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (3.19.3)\n",
      "Requirement already satisfied: traits>=6.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (6.4.3)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (3.15.3)\n",
      "Requirement already satisfied: acres in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (0.1.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: puremagic in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nipype->fitz) (1.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: lxml>=4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyxnat->fitz) (5.3.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyxnat->fitz) (2.32.3)\n",
      "Requirement already satisfied: pathlib>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: ci-info>=0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->pyxnat->fitz) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fitz\n",
    "!pip3 install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Mahesh_Duddu_DS-2YoE (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/kathik-resume.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Ranjith_Kumar_KN_2024.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Kshitij_Chaudhari (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Vedant_Mahalle_Resume_2023 (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/wasif (AI_ML-3-YOE).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/ResumeNM.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Akshat_Agarwal_Resume_CV.docx (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Saurav_Kumar_Resume.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/VINEETH_resume (2) (2) (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Manish Kumar_Gen AI_Apna (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Bindu_Madhavi_Vempati_resume.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Ali Nasir_Gen AI_Apna (1) (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Manasi_Khillare_CV (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Pratik_Behera_Resume.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Resume_VISHAL_SHARMA.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Ramyashree G CV.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Sunny-Dhoke.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Syed Resume2.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Resume-Aditya-PDF_compressed.txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/ANUP_RESUME_UPDATED (1) (1).txt\n",
      "Extracted text saved to /Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Resume_Aarush_Gupta.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  \n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing unwanted characters and punctuation.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'[_/]', '', text)    \n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = []\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:\n",
    "            if b['type'] == 0:  \n",
    "                for line in b[\"lines\"]:\n",
    "                    sentence = []\n",
    "                    for span in line[\"spans\"]:\n",
    "                        cleaned_text = clean_text(span['text'])\n",
    "                        sentence.append(cleaned_text)\n",
    "                    if sentence:\n",
    "                        full_text.append(' '.join(sentence))\n",
    "\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def process_pdf_folder(source_folder, target_folder):\n",
    "    \"\"\"Process all PDFs in the source folder and save their texts to the target folder.\"\"\"\n",
    "    os.makedirs(target_folder, exist_ok=True)  \n",
    "    \n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(source_folder, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            output_filename = filename.replace('.pdf', '.txt')\n",
    "            output_path = os.path.join(target_folder, output_filename)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            print(f\"Extracted text saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_folder = '/Users/0xr4plh/Documents/NLP_Assignment_2jt/GenAI_Experienced_Candidates_Profiles'\n",
    "    target_folder = '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes'\n",
    "    process_pdf_folder(source_folder, target_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/0xr4plh/Documents/NLP_Assignment_2jt/company's_requirements.txt\",\"r\", encoding=\"utf-8\") as file:\n",
    "    JD = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "instructions = \"\"\"You will be provided with a Job description of a company , more precisely \"What company is looking for\" section , based on this text you have to output a python list containing semantically most relavent skills, by semantically it is meant that even though , the skills are not directly listed in the text provided , but is essential as far as \"What company is looking for\" section is concered. Any relavent keyword from this text which is directly not considered as skill , but would be important keyword when the screening of candidates resumes will be done using this python list that you will be generating , example of such keywords are - [fine-tuning , researching , transformers , deep learning , machine learning , nlp , cloud , agents ] , all these are essential keywords which directly are not considered as skills , but are essential when the keyword maching would be done to candidate's resume. Just output the python list named output : [contents extracted] , nothing else should be there in the output.\"\"\"\n",
    "\n",
    "example = \"\"\"Example 1 - Input : \n",
    " We are seeking passionate and talented freshers with a strong foundation in coding.\n",
    " Eagerness to delve into systems architecture, especially in the data, AI, and ML\n",
    "Domains.\n",
    " Basic understanding of Nvidia CUDA and custom AI/ML libraries.\n",
    " Exposure to modelling techniques, including pre-training and fine-tuning\n",
    "Foundational models.\n",
    " Proficiency in Python programming and familiarity with frameworks like PyTorch.\n",
    " Keenness to learn and adapt to cloud platforms like AWS, GCP, or Azure,\n",
    "alongside exposure to on-premises and colocation service hosting environments.\n",
    " Familiarity with at least one system language such as C, C++, C#, Go, Rust, Java, or\n",
    "Scala.\n",
    "Output - [AI , ML , CUDA , Fine-tuning , data , pre-training , python , Pytorch , cloud , AWS , GCP , Azure , C, C++, C#, Go, Rust, Java,Scala]\n",
    "\n",
    "Example 2 - Input :\n",
    "- Profound understanding of Deep Learning, NLP, and the ability to build or debug LLM and NLP models.\n",
    "- Extensive knowledge of transformers, and familiarity with libraries such as PyTorch, CUDA, bitsandbytes, and TensorFlow.\n",
    "- Expertise in optimization methods (e.g., quantization), training methods (e.g., LoRA), and evaluation techniques for LLMs.\n",
    "- Expertise with managing and training LLM checkpoints on multiple GPUs in parallel.\n",
    "- Strong skills in working with Linux systems, especially on cloud HPC clusters (like AWS) or environments with multiple GPUs.\n",
    "- Proficiency in Python, deep learning and parallel computing libraries suitable for a Linux environment with multiple GPUs.\n",
    "- Familiarity with advanced programming frameworks like llama.cpp, langchain (not mandatory).\n",
    "- Background knowledge in simulations, robotics, reinforcement learning, or intelligent agents within simulated environments such as game engines is advantageous.\n",
    "Output - [Deep learning , nlp , llm , transformers , Pytorch , CUDA , bitsandbytes , TensorFlow , quantization , LoRA , GPUs , training , Linux , cloud , AWS , HPC clusters , Python , llama , langchain , simulations , robortics , reinforcement leanring , agents ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(example + instructions + JD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "instructions = \"\"\" You will be provided the resume text of candidate , based on the candidate's resume text you have to output a python list which will list down all the skills written down in the text , and the skills of the candidate listed down should be semantically captured , by semantically it is meant that even though , the skills are not directly listed in the text provided , but is essential as far as company's job requirements is concerned. Any relavent keyword from this candidate's resume text which is directly not considered as skill should be listed down in the list that you will be generating because when the keyword matching of this list that you will be generating would be done against the list of skills required by the company and the list of the company's skills is also semantically prepared the screening of candidate's resumes.  example of such keywords are - [fine-tuning , researching , transformers , deep learning , machine learning , nlp , cloud , agents ] , all these are essential keywords which directly are not considered as skills. If candidate has listed multiple languages , then give all the languages in a list inside the main python list. Semantically list down all the relevant keywords in the resume text which could have potential match with the keywords presnet in the list of company's requirement skills and capture all unique catchy keywords present in the resume. Just output the python list named output : [contents extracted] , nothing else should be there in the output. output = [.....] , just this.\"\"\"\n",
    "\n",
    "\n",
    "example = \"\"\"\n",
    "\n",
    "Example 1 : Input : \n",
    "\n",
    "Akshat Agarwal\n",
    "Machine Learning Engineer\n",
    "Delhi India\n",
    "akshatagarwal0311gmailcom\n",
    "8447701604\n",
    "httpswwwlinkedincominakshat03\n",
    "QUALIFICATION SUMMARY\n",
    "Experienced Applied Research Scientist specializing in software development and machine\n",
    "learning with expertise in AI research and transitioning research to production Skilled in leading\n",
    "technical teams in startup environments and developing AIdriven solutions for Intelligent\n",
    "Document Processing IDP Automatic Speech Recognition ASR and Natural Language\n",
    "Understanding NLU using advanced Deep Learning techniques and programming skills\n",
    "KEY SKILLS\n",
    "\n",
    "Python\n",
    "\n",
    "Deep Learning\n",
    "\n",
    "Pytorch\n",
    "\n",
    "Github\n",
    "\n",
    "Kubernetes\n",
    "\n",
    "Computer Vision\n",
    "PROFESSIONAL EXPERIENCE\n",
    "Senior Machine Learning Engineer\n",
    "Feb23 Current\n",
    "NeuralSpace\n",
    "Bangalore India\n",
    "\n",
    "Spearheaded the development of an advanced IDP platform capable of processing both\n",
    "structured and unstructured documents in English and Arabic\n",
    "\n",
    "Integrated stateoftheart models and frameworks such as  Pix2Struct   LLaVA LLM  and\n",
    "YOLO  to significantly enhance  Visual Question Answering VQA   OCR  and  Object\n",
    "Detection  capabilities\n",
    "\n",
    "Designed and trained novel  VQA  and  OCR  pipelines by creating a proprietary inhouse\n",
    "dataset achieving  stateoftheart  performance in document understanding tasks for\n",
    "Arabic\n",
    "\n",
    "Successfully deployed the IDP platform using  Kubernetes  ensuring scalability and\n",
    "reliability across diverse production environments optimized for performance\n",
    "Applied Research Scientist\n",
    "June21 Jan23\n",
    "NeuralSpace\n",
    "Bangalore India\n",
    "\n",
    "Led the research and development of  SpeechToText  STT  Natural Language\n",
    "Understanding  NLU models signiﬁcantly improving linguistic accessibility and user\n",
    "interaction across multiple languages\n",
    "\n",
    "Leveraged frameworks like  Whisper   Kaldi  and  wav2vec  for speech processing and\n",
    "transformer architectures  for NLU achieving cuttingedge performance in multilingual\n",
    "environments\n",
    "Applied Research Scientist Intern\n",
    "Nov20  May21\n",
    "NeuralSpace\n",
    "Remote\n",
    "\n",
    "Designed and implemented  NLU  frameworks for lowresource languages\n",
    "\n",
    "Developed  Transliteration  models using Pytorch for 12 Indian languages\n",
    "\n",
    "Trained  Named Entity Recognition  NER models for 55 lowresource languages for\n",
    "NeuralSpaces TextAI platform\n",
    "EDUCATION\n",
    "BTech in Electrical\n",
    "20172021\n",
    "Delhi Technological University\n",
    "Delhi India\n",
    "CGPA 819\n",
    "AISSCECBSE Class XII\n",
    "2017\n",
    "Sardar Patel Vidyalaya\n",
    "Delhi India\n",
    "Percentage  942\n",
    "PUBLICATIONS\n",
    "SustaiNLP  EMNLP20\n",
    "End to End Binarized Neural Networks for Text Classiﬁcation\n",
    "\n",
    "Constructed a fully  Binarized Neural Network pipeline  for the task of i ntent classiﬁcation\n",
    "using  CNN  architecture\n",
    "\n",
    "Weights and activations of network layers were constricted to  11  which are highly\n",
    "efﬁcient in terms of hardware and memory\n",
    "\n",
    "Achieved  stateoftheart  results on various datasets while  reducing memory footprint\n",
    "and training time  due to effectiveness of binary operations\n",
    "Hasoc2021\n",
    "One to Rule Them All Towards Joint Indic Language Hate Speech\n",
    "Detection\n",
    "\n",
    "Presented a  multilingual architecture  using stateoftheart transformer language models\n",
    "to jointly learn  hate and offensive speech detection  across three languages ie English\n",
    "Hindi and Marathi\n",
    "\n",
    "On the provided testing corpora achieved  high accuracy  showing the efficacy of\n",
    "exploiting a  multilingual training scheme \n",
    "CERTIFICATIONS\n",
    "Deep Learning Specialization\n",
    "deeplearningai\n",
    "Machine Learning with Python\n",
    "IBM\n",
    "Python for Data Science and Machine Learning Bootcamp\n",
    "Udemy\n",
    "\n",
    "output - [Machine Leanring , AI , research , Intelligent Document Processing , Automatic speech recognition (ASR) , natural language understanding , deep learning , Python ,Pytorch , Github , Kubernetes , Computer Vision , LLM , YOLO , Visual Question Answering(VQA) , OCR , Object detection , Speech-To-Text(STT) , Whisper , Kaldi , wav2vec , speech-processing , transofrmers , named entity recognition(NER) , Neural Networks , Text Classification , intent classification , CNN , multilingual architecture , data science]\n",
    "\n",
    "Input : \n",
    "\"\"\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "folder_path = \"/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for filename in os.listdir(\"/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes\"):\n",
    "    filepath = os.path.join(folder_path,filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        paths.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to api call limit , already first 16 resume text is extracted and rest 4 are from here\n",
    "for i in range(17,len(paths)):\n",
    "    with open(paths[i],\"r\",encoding=\"utf-8\") as file:\n",
    "        resume_text = file.read()\n",
    "        response = model.generate_content(example + instructions + resume_text)\n",
    "        results[os.listdir(folder_path)[i]] = response.text[10:len(response.text)-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/0xr4plh/Documents/NLP_Assignment_2jt/Candidates_Skills.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON data has been successfully updated with lowercase elements.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'Candidates_Skills.json'\n",
    "\n",
    "def to_lowercase(item):\n",
    "    if isinstance(item, list):\n",
    "        return [to_lowercase(subitem) for subitem in item]\n",
    "    elif isinstance(item, str):\n",
    "        return item.lower()\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for key in data.keys():\n",
    "    data[key] = to_lowercase(data[key])\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"The JSON data has been successfully updated with lowercase elements.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON data has been successfully updated with lowercase elements.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = 'JD_skills_list.json'\n",
    "\n",
    "def to_lowercase(item):\n",
    "    if isinstance(item, list):\n",
    "        return [to_lowercase(subitem) for subitem in item]\n",
    "    elif isinstance(item, str):\n",
    "        return item.lower()\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for key in data.keys():\n",
    "    data[key] = to_lowercase(data[key])\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"The JSON data has been successfully updated with lowercase elements.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to JD_skills_list.py\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "json_file_path = '/Users/0xr4plh/Documents/NLP_Assignment_2jt/JD_skills_list.json'\n",
    "\n",
    "output_python_file = 'JD_skills_list.py'\n",
    "\n",
    "def convert_json_lists(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    converted_data = {}\n",
    "    for key, value in data.items():\n",
    "        converted_data[key] = ast.literal_eval(value)\n",
    "\n",
    "    return converted_data\n",
    "\n",
    "converted_data = convert_json_lists(json_file_path)\n",
    "\n",
    "def save_to_python_file(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"# This file was generated automatically and contains candidates' skills as lists.\\n\")\n",
    "        file.write(\"candidates_skills = \")\n",
    "        file.write(json.dumps(data, indent=4)) \n",
    "\n",
    "save_to_python_file(converted_data, output_python_file)\n",
    "\n",
    "print(f\"Data has been saved to {output_python_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Candidates_Skills import candidates_skills\n",
    "from JD_skills_list import Job_D\n",
    "\n",
    "jd_skills = Job_D[\"JD\"]\n",
    "\n",
    "resume_scores_keyword_search = {}\n",
    "\n",
    "for candidate, skills in candidates_skills.items():\n",
    "    matched_skills = [skill for skill in jd_skills if skill in skills]\n",
    "    score = (len(matched_skills) / len(jd_skills)) * 100\n",
    "    resume_scores_keyword_search[candidate] = score\n",
    "\n",
    "with open('Resume_Scores.py', 'w') as file:\n",
    "    file.write(f\"resume_scores_keyword_search = {resume_scores_keyword_search}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Candidates_Skills import candidates_skills\n",
    "from Resume_Scores import resume_scores_keyword_search\n",
    "from JD_skills_list import Job_D\n",
    "\n",
    "jd_skills = Job_D[\"JD\"]\n",
    "\n",
    "unmatched_skills = {}\n",
    "\n",
    "for candidate, skills in candidates_skills.items():\n",
    "    matched_skills = [skill for skill in jd_skills if skill in skills]\n",
    "    non_matched_skills = [skill for skill in skills if skill not in matched_skills]\n",
    "    unmatched_skills[candidate] = non_matched_skills\n",
    "\n",
    "with open('soft_search_skills.py', 'w') as file:\n",
    "    file.write(f\"unmatched_skills = {unmatched_skills}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m input_to_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m company\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms_requirement_skills_list - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_requirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m candidate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms_skills_list - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidate_skills_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Call the generative model to get the score\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Extract the score from response\u001b[39;00m\n\u001b[1;32m    180\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)  \u001b[38;5;66;03m# Assuming the model outputs in the format \"Output: 90\"\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "from JD_skills_list import Job_D as company_requirement_skills_list\n",
    "from soft_search_skills import unmatched_skills\n",
    "import json\n",
    "\n",
    "genai.configure(api_key=\"\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "instructions = \"\"\" You are a skill analyser expert. You will be given 2 lists - 1. comapny's requirement skills list and 2. candidate's skills list , based on skills of the candidate you have to evaluate and analyse that how fit is candidate's skills as far as company's requirement is concered. Based on your analysis you just have to output a score out of 100 to the candidate. You would not find the any common keyword matching betwwen the two lists (no hard search but it may happen like candidate's skills list have keywords like nlp , cv and in company's requirement is natural language processing , computer vision or vice versa which makes candidate 100% fit , so also consider this as a factor in your score , be aware of contextual short-forms and full-forms used), so you don't have to do hard search instead you have to semantically capture and analyse how fit is the candidate as far as company's requirement is concerned. If the skills of the candidate are more advanced with respect to company's requirement you will give a good score , if the skills of the candidate are basic and less advanced with respect to company's requirement then you will give low score , if the skills of the candidate is of same level with respect to company's requirement then you will give a neural score.You have to give a low score if candidate know advanced skills but it's of not of much use of company's requirement. Based on your knowledge and semantic analysis you just have to give a score out from 100 , nothing else in the output.  \"\"\"\n",
    "\n",
    "example = \"\"\"\n",
    "Example 1 -\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - [\n",
    "    \"Data Structuring and Algo\",\n",
    "    \"DSA\", \n",
    "    \"Modularized Programming\", \n",
    "    \"Optimized Code Design\", \n",
    "    \"Time Series Forecasting\",\n",
    "    \"Natural Language Understanding (NLU)\", \n",
    "    \"Transformer Architectures\", \n",
    "    \"Large Language Model Optimization\", \n",
    "    \"Diffusion Models\", \n",
    "    \"Generative Pre-trained Transformers (GPT)\", \n",
    "    \"Attention Mechanisms\", \n",
    "    \"Neural Architecture Search (NAS)\", \n",
    "    \"Reinforcement Learning with Human Feedback (RLHF)\", \n",
    "    \"Few-shot Learning\", \n",
    "    \"Zero-shot Learning\", \n",
    "    \"Multi-modal AI\", \n",
    "    \"Self-supervised Learning\", \n",
    "    \"Contrastive Learning\", \n",
    "    \"Prompt Engineering\", \n",
    "    \"Scalable Deep Learning Systems\"\n",
    "]\n",
    "\n",
    "Output : 90 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - candidate_skills = [\n",
    "    \"Data Structuring \",\n",
    "    \"Basic DSA\", \n",
    "    \"Simple Modular Code\", \n",
    "    \"Code Optimization Techniques\", \n",
    "    \"Intro to Time Series Analysis\",\n",
    "    \"Natural Language Processing Basics\", \n",
    "    \"Neural Networks\", \n",
    "    \"Basic Generative Models\", \n",
    "    \"Text Processing Techniques\", \n",
    "    \"RNNs and LSTMs\", \n",
    "    \"Feature Engineering\", \n",
    "    \"Bag of Words (BoW)\", \n",
    "    \"TF-IDF\", \n",
    "    \"Basic Machine Translation\", \n",
    "    \"Clustering Algorithms\", \n",
    "    \"Supervised Learning Methods\", \n",
    "    \"Unsupervised Learning\", \n",
    "    \"Hyperparameter Tuning\", \n",
    "    \"Model Evaluation Techniques\", \n",
    "    \"Data Cleaning and Preprocessing\"\n",
    "]\n",
    "\n",
    "\n",
    "Output - 35 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Java', 'C', 'C++', 'object-oriented design', 'design patterns', 'MySQL', 'NoSQL', 'HBase', 'Elasticsearch', 'Aerospike'] ,\n",
    "candidate's_skills_list - [\n",
    "    'Kotlin',\n",
    "    'Python',\n",
    "    'GoLang',\n",
    "    'TypeScript',\n",
    "    'functional programming concepts',\n",
    "    'domain-driven design',\n",
    "    'PostgreSQL',\n",
    "    'MongoDB',\n",
    "    'Cassandra',\n",
    "    'Redis',\n",
    "    'DynamoDB',\n",
    "    'clean architecture principles',\n",
    "    'microservices architecture',\n",
    "    'TDD',\n",
    "    'GraphQL',\n",
    "    'Flask',\n",
    "    'Azure',\n",
    "    'CI/CD pipelines',\n",
    "    'containerization with Docker',\n",
    "    'Kubernetes',\n",
    "    'GitLab CI'\n",
    "]\n",
    "\n",
    "Output -  70,\n",
    "\n",
    "company's_requirement_skills_list - ['Java', 'C', 'C++', 'object-oriented design', 'design patterns', 'MySQL', 'NoSQL', 'HBase', 'Elasticsearch', 'Aerospike'] ,\n",
    "candidate's_skills_list - [\n",
    "    'HTML',\n",
    "    'CSS',\n",
    "    'JavaScript',\n",
    "    'object-oriented principles',\n",
    "    'basic database queries',\n",
    "    'SQLite',\n",
    "    'JSON handling',\n",
    "    'basic REST APIs',\n",
    "    'data modeling',\n",
    "    'CRUD operations',\n",
    "    'debugging skills',\n",
    "    'version control with Git',\n",
    "    'shell scripting',\n",
    "    'file handling in Python',\n",
    "    'introductory SQL commands',\n",
    "    'modular programming basics',\n",
    "    'loops and conditionals',\n",
    "    'array manipulations',\n",
    "    'intro to Agile methodology'\n",
    "]\n",
    "\n",
    "Output - 10 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - [\n",
    "    'Kotlin',\n",
    "    'Rust',\n",
    "    'TypeScript',\n",
    "    'PostgreSQL',\n",
    "    'MongoDB',\n",
    "    'Redis',\n",
    "    'Cassandra',\n",
    "    'Apache Zookeeper',\n",
    "    'GraphQL',\n",
    "    'microservices frameworks (Spring Boot, Flask)',\n",
    "    'Docker',\n",
    "    'Kubernetes',\n",
    "    'Azure Cloud Services',\n",
    "    'GCP services',\n",
    "    'Ansible',\n",
    "    'unit testing (JUnit, Pytest)',\n",
    "    'design principles (GRASP, SOLID)',\n",
    "    'code versioning (Git, SVN)',\n",
    "    'distributed system design',\n",
    "    'API gateways and load balancers'\n",
    "]\n",
    "\n",
    "Output - 75 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - [\n",
    "    'Quantum Computing',\n",
    "    'Blockchain Technology',\n",
    "    'Augmented Reality Development',\n",
    "    'Cybersecurity Measures',\n",
    "    'IoT Infrastructure',\n",
    "    'Cloud Architecture',\n",
    "    'Big Data Analytics',\n",
    "    '3D Modeling',\n",
    "    'Virtual Reality Integration',\n",
    "    'Embedded Systems'\n",
    "\n",
    "]\n",
    "\n",
    "Output - 20\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "resume_score_semantic_search = {}\n",
    "\n",
    "\n",
    "for candidate, skills in unmatched_skills.items():\n",
    "    candidate_skills_list = str(skills) \n",
    "    company_requirement = str(company_requirement_skills_list)  \n",
    "    \n",
    "\n",
    "    input_to_model = f\"{example} {instructions} company's_requirement_skills_list - {company_requirement} candidate's_skills_list - {candidate_skills_list}\"\n",
    "    \n",
    "\n",
    "    response = model.generate_content(input_to_model)\n",
    "\n",
    "    score = float(response.text)  \n",
    "    resume_score_semantic_search[candidate] = score\n",
    "\n",
    "with open('Resume_Scores.py', 'a') as file:\n",
    "    file.write(f\"\\nresume_score_semantic_search = {json.dumps(resume_score_semantic_search)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/ResumeNM.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Akshat_Agarwal_Resume_CV.docx (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Vedant_Mahalle_Resume_2023 (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/wasif (AI_ML-3-YOE).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Ali Nasir_Gen AI_Apna (1) (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Saurav_Kumar_Resume.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/VINEETH_resume (2) (2) (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Manish Kumar_Gen AI_Apna (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Bindu_Madhavi_Vempati_resume.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Mahesh_Duddu_DS-2YoE (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Kshitij_Chaudhari (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/kathik-resume.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Ranjith_Kumar_KN_2024.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Sunny-Dhoke.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/ANUP_RESUME_UPDATED (1) (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Resume_Aarush_Gupta.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Syed Resume2.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Resume-Aditya-PDF_compressed.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Pratik_Behera_Resume.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Manasi_Khillare_CV (1).txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Ramyashree G CV.txt',\n",
       " '/Users/0xr4plh/Documents/NLP_Assignment_2jt/Extracted_text_of_resumes/Resume_VISHAL_SHARMA.txt']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "instructions = \"\"\" You are a skill analyser expert. You will be given 2 lists - 1. comapny's requirement skills list and 2. candidate's skills list , based on skills of the candidate you have to evaluate and analyse that how fit is candidate's skills as far as company's requirement is concered. Based on your analysis you just have to output a score out of 100 to the candidate. You would not find the any common keyword matching betwwen the two lists (no hard search but it may happen like candidate's skills list have keywords like nlp , cv and in company's requirement is natural language processing , computer vision or vice versa which makes candidate 100% fit , so also consider this as a factor in your score , be aware of contextual short-forms and full-forms used), so you don't have to do hard search instead you have to semantically capture and analyse how fit is the candidate as far as company's requirement is concerned. If the skills of the candidate are more advanced with respect to company's requirement you will give a good score , if the skills of the candidate are basic and less advanced with respect to company's requirement then you will give low score , if the skills of the candidate is of same level with respect to company's requirement then you will give a neural score.You have to give a low score if candidate know advanced skills but it's of not of much use of company's requirement. Based on your knowledge and semantic analysis you just have to give a score out from 100 , nothing else in the output.  \"\"\"\n",
    "\n",
    "example = \"\"\"\n",
    "Example 1 -\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - [\n",
    "    \"Data Structuring and Algo\",\n",
    "    \"DSA\", \n",
    "    \"Modularized Programming\", \n",
    "    \"Optimized Code Design\", \n",
    "    \"Time Series Forecasting\",\n",
    "    \"Natural Language Understanding (NLU)\", \n",
    "    \"Transformer Architectures\", \n",
    "    \"Large Language Model Optimization\", \n",
    "    \"Diffusion Models\", \n",
    "    \"Generative Pre-trained Transformers (GPT)\", \n",
    "    \"Attention Mechanisms\", \n",
    "    \"Neural Architecture Search (NAS)\", \n",
    "    \"Reinforcement Learning with Human Feedback (RLHF)\", \n",
    "    \"Few-shot Learning\", \n",
    "    \"Zero-shot Learning\", \n",
    "    \"Multi-modal AI\", \n",
    "    \"Self-supervised Learning\", \n",
    "    \"Contrastive Learning\", \n",
    "    \"Prompt Engineering\", \n",
    "    \"Scalable Deep Learning Systems\"\n",
    "]\n",
    "\n",
    "Output : 90 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - candidate_skills = [\n",
    "    \"Data Structuring \",\n",
    "    \"Basic DSA\", \n",
    "    \"Simple Modular Code\", \n",
    "    \"Code Optimization Techniques\", \n",
    "    \"Intro to Time Series Analysis\",\n",
    "    \"Natural Language Processing Basics\", \n",
    "    \"Neural Networks\", \n",
    "    \"Basic Generative Models\", \n",
    "    \"Text Processing Techniques\", \n",
    "    \"RNNs and LSTMs\", \n",
    "    \"Feature Engineering\", \n",
    "    \"Bag of Words (BoW)\", \n",
    "    \"TF-IDF\", \n",
    "    \"Basic Machine Translation\", \n",
    "    \"Clustering Algorithms\", \n",
    "    \"Supervised Learning Methods\", \n",
    "    \"Unsupervised Learning\", \n",
    "    \"Hyperparameter Tuning\", \n",
    "    \"Model Evaluation Techniques\", \n",
    "    \"Data Cleaning and Preprocessing\"\n",
    "]\n",
    "\n",
    "\n",
    "Output - 35 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Java', 'C', 'C++', 'object-oriented design', 'design patterns', 'MySQL', 'NoSQL', 'HBase', 'Elasticsearch', 'Aerospike'] ,\n",
    "candidate's_skills_list - [\n",
    "    'Kotlin',\n",
    "    'Python',\n",
    "    'GoLang',\n",
    "    'TypeScript',\n",
    "    'functional programming concepts',\n",
    "    'domain-driven design',\n",
    "    'PostgreSQL',\n",
    "    'MongoDB',\n",
    "    'Cassandra',\n",
    "    'Redis',\n",
    "    'DynamoDB',\n",
    "    'clean architecture principles',\n",
    "    'microservices architecture',\n",
    "    'TDD',\n",
    "    'GraphQL',\n",
    "    'Flask',\n",
    "    'Azure',\n",
    "    'CI/CD pipelines',\n",
    "    'containerization with Docker',\n",
    "    'Kubernetes',\n",
    "    'GitLab CI'\n",
    "]\n",
    "\n",
    "Output -  70,\n",
    "\n",
    "company's_requirement_skills_list - ['Java', 'C', 'C++', 'object-oriented design', 'design patterns', 'MySQL', 'NoSQL', 'HBase', 'Elasticsearch', 'Aerospike'] ,\n",
    "candidate's_skills_list - [\n",
    "    'HTML',\n",
    "    'CSS',\n",
    "    'JavaScript',\n",
    "    'object-oriented principles',\n",
    "    'basic database queries',\n",
    "    'SQLite',\n",
    "    'JSON handling',\n",
    "    'basic REST APIs',\n",
    "    'data modeling',\n",
    "    'CRUD operations',\n",
    "    'debugging skills',\n",
    "    'version control with Git',\n",
    "    'shell scripting',\n",
    "    'file handling in Python',\n",
    "    'introductory SQL commands',\n",
    "    'modular programming basics',\n",
    "    'loops and conditionals',\n",
    "    'array manipulations',\n",
    "    'intro to Agile methodology'\n",
    "]\n",
    "\n",
    "Output - 10 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - [\n",
    "    'Kotlin',\n",
    "    'Rust',\n",
    "    'TypeScript',\n",
    "    'PostgreSQL',\n",
    "    'MongoDB',\n",
    "    'Redis',\n",
    "    'Cassandra',\n",
    "    'Apache Zookeeper',\n",
    "    'GraphQL',\n",
    "    'microservices frameworks (Spring Boot, Flask)',\n",
    "    'Docker',\n",
    "    'Kubernetes',\n",
    "    'Azure Cloud Services',\n",
    "    'GCP services',\n",
    "    'Ansible',\n",
    "    'unit testing (JUnit, Pytest)',\n",
    "    'design principles (GRASP, SOLID)',\n",
    "    'code versioning (Git, SVN)',\n",
    "    'distributed system design',\n",
    "    'API gateways and load balancers'\n",
    "]\n",
    "\n",
    "Output - 75 ,\n",
    "\n",
    "company's_requirement_skills_list - ['Python', 'R', 'SQL', 'Hive', 'Spark', 'Time Series', 'NLP', 'Deep Learning', 'LLMs', 'Generative AI', 'data structures', 'algorithms', 'modular coding', 'optimized code']\n",
    "candidate's_skills_list - [\n",
    "    'Quantum Computing',\n",
    "    'Blockchain Technology',\n",
    "    'Augmented Reality Development',\n",
    "    'Cybersecurity Measures',\n",
    "    'IoT Infrastructure',\n",
    "    'Cloud Architecture',\n",
    "    'Big Data Analytics',\n",
    "    '3D Modeling',\n",
    "    'Virtual Reality Integration',\n",
    "    'Embedded Systems'\n",
    "\n",
    "]\n",
    "\n",
    "Output - 20\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Prompt user for input\n",
    "company_requirement_skills_list = input(\"Enter the company's_requirement_skills_list: \")\n",
    "\n",
    "candidate_skills_list = input(\"Enter the candidate_skills_list: \")\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content(example + instructions + \"company's_requirement_skills_list - \" + company_requirement_skills_list + \" \" + \"candidate's_skills_list - \" + candidate_skills_list)\n",
    "\n",
    "# Print the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append or create the semantic search scores in Resume_Scores.py\n",
    "with open('Resume_Scores.py', 'a') as file:\n",
    "    file.write(f\"\\nresume_score_semantic_search = {json.dumps(resume_score_semantic_search)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResumeNM.txt': 55.0, 'Akshat_Agarwal_Resume_CV.docx (1).txt': 49.0, 'Vedant_Mahalle_Resume_2023 (1).txt': 61.0, 'wasif (AI_ML-3-YOE).txt': 73.0, 'Ali Nasir_Gen AI_Apna (1) (1).txt': 64.0, 'Saurav_Kumar_Resume.txt': 42.0, 'VINEETH_resume (2) (2) (1).txt': 48.0, 'Manish Kumar_Gen AI_Apna (1).txt': 58.0, 'Bindu_Madhavi_Vempati_resume.txt': 58.0, 'Mahesh_Duddu_DS-2YoE (1).txt': 52.0, 'Kshitij_Chaudhari (1).txt': 56.0, 'kathik-resume.txt': 47.0, 'Ranjith_Kumar_KN_2024.txt': 67.0, 'Sunny-Dhoke.txt': 55.0, 'ANUP_RESUME_UPDATED (1) (1).txt': 52.0, 'Resume_Aarush_Gupta.txt': 52.0, 'Syed Resume2.txt': 59.0, 'Resume-Aditya-PDF_compressed.txt': 48.0, 'Pratik_Behera_Resume.txt': 49.0, 'Manasi_Khillare_CV (1).txt': 49.0, 'Ramyashree G CV.txt': 59.0, 'Resume_VISHAL_SHARMA.txt': 49.0}\n"
     ]
    }
   ],
   "source": [
    "resume_scores_keyword_search = {'ResumeNM.txt': 35.0, 'Akshat_Agarwal_Resume_CV.docx (1).txt': 25.0, 'Vedant_Mahalle_Resume_2023 (1).txt': 45.0, 'wasif (AI_ML-3-YOE).txt': 65.0, 'Ali Nasir_Gen AI_Apna (1) (1).txt': 50.0, 'Saurav_Kumar_Resume.txt': 20.0, 'VINEETH_resume (2) (2) (1).txt': 30.0, 'Manish Kumar_Gen AI_Apna (1).txt': 40.0, 'Bindu_Madhavi_Vempati_resume.txt': 40.0, 'Mahesh_Duddu_DS-2YoE (1).txt': 30.0, 'Kshitij_Chaudhari (1).txt': 30.0, 'kathik-resume.txt': 15.0, 'Ranjith_Kumar_KN_2024.txt': 55.0, 'Sunny-Dhoke.txt': 35.0, 'ANUP_RESUME_UPDATED (1) (1).txt': 30.0, 'Resume_Aarush_Gupta.txt': 30.0, 'Syed Resume2.txt': 35.0, 'Resume-Aditya-PDF_compressed.txt': 30.0, 'Pratik_Behera_Resume.txt': 25.0, 'Manasi_Khillare_CV (1).txt': 25.0, 'Ramyashree G CV.txt': 35.0, 'Resume_VISHAL_SHARMA.txt': 25.0}\n",
    "resume_score_semantic_search = {\"ResumeNM.txt\": 85.0, \"Akshat_Agarwal_Resume_CV.docx (1).txt\": 85.0, \"Vedant_Mahalle_Resume_2023 (1).txt\": 85.0, \"wasif (AI_ML-3-YOE).txt\": 85.0, \"Ali Nasir_Gen AI_Apna (1) (1).txt\": 85.0, \"Saurav_Kumar_Resume.txt\": 75.0, \"VINEETH_resume (2) (2) (1).txt\": 75.0, \"Manish Kumar_Gen AI_Apna (1).txt\": 85.0, \"Bindu_Madhavi_Vempati_resume.txt\": 85.0, \"Mahesh_Duddu_DS-2YoE (1).txt\": 85.0, \"Kshitij_Chaudhari (1).txt\": 95.0, \"kathik-resume.txt\": 95.0, \"Ranjith_Kumar_KN_2024.txt\": 85.0, \"Sunny-Dhoke.txt\": 85.0, \"ANUP_RESUME_UPDATED (1) (1).txt\": 85.0, \"Resume_Aarush_Gupta.txt\": 85.0, \"Syed Resume2.txt\": 95.0, \"Resume-Aditya-PDF_compressed.txt\": 75.0, \"Pratik_Behera_Resume.txt\": 85.0, \"Manasi_Khillare_CV (1).txt\": 85.0, \"Ramyashree G CV.txt\": 95.0, \"Resume_VISHAL_SHARMA.txt\": 85.0}\n",
    "\n",
    "net_resume_scores = {}\n",
    "\n",
    "for key in resume_scores_keyword_search:\n",
    "    keyword_score = resume_scores_keyword_search[key]\n",
    "    semantic_score = resume_score_semantic_search[key]\n",
    "    weighted_average = 0.6 * keyword_score + 0.4 * semantic_score\n",
    "    net_resume_scores[key] = weighted_average\n",
    "\n",
    "print(net_resume_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResumeNM.txt': 55.0,\n",
       " 'Akshat_Agarwal_Resume_CV.docx (1).txt': 49.0,\n",
       " 'Vedant_Mahalle_Resume_2023 (1).txt': 61.0,\n",
       " 'wasif (AI_ML-3-YOE).txt': 73.0,\n",
       " 'Ali Nasir_Gen AI_Apna (1) (1).txt': 64.0,\n",
       " 'Saurav_Kumar_Resume.txt': 42.0,\n",
       " 'VINEETH_resume (2) (2) (1).txt': 48.0,\n",
       " 'Manish Kumar_Gen AI_Apna (1).txt': 58.0,\n",
       " 'Bindu_Madhavi_Vempati_resume.txt': 58.0,\n",
       " 'Mahesh_Duddu_DS-2YoE (1).txt': 52.0,\n",
       " 'Kshitij_Chaudhari (1).txt': 56.0,\n",
       " 'kathik-resume.txt': 47.0,\n",
       " 'Ranjith_Kumar_KN_2024.txt': 67.0,\n",
       " 'Sunny-Dhoke.txt': 55.0,\n",
       " 'ANUP_RESUME_UPDATED (1) (1).txt': 52.0,\n",
       " 'Resume_Aarush_Gupta.txt': 52.0,\n",
       " 'Syed Resume2.txt': 59.0,\n",
       " 'Resume-Aditya-PDF_compressed.txt': 48.0,\n",
       " 'Pratik_Behera_Resume.txt': 49.0,\n",
       " 'Manasi_Khillare_CV (1).txt': 49.0,\n",
       " 'Ramyashree G CV.txt': 59.0,\n",
       " 'Resume_VISHAL_SHARMA.txt': 49.0}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_resume_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
